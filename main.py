# -*- coding: utf-8 -*-
"""CS 480 - Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yx2wfm5svxzoe8Rv7PeulQ853shPklLe

# Import and setup Kaggle data sources
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'cs-480-2024-spring:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F81655%2F8915386%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240812%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240812T025217Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D71b4e54c8b99120fa037c2f4d3fcc954a957c02cab223bb17a2ba05e95a5c729ca3f5604f5ce704be44a86059e2012213343d5c5bab84daba0207501dea93f4595d6517b8556ded4fb91e7577b4c73e8fa816205b9aee839668924d2f350ccb624d68795d4a4253f6c97f36740fc8e6cc0466827e615a178db6af47b71a9bba0854a9490593e36ba6042df7aaa610ff8642140e610844cc2ac614d2c6fa036564452d6b12be1316d1af69140c53647de2992af7b5c50762798e6999252dead3efba4117ec6f51ff4f16e07c0ff8052e11a023ab430b9ae89e91344d680742bcec035e79ac7d51dc01da206d471a31ec7fbf4669ef3ac4ab9bf89c169ef37e236,dinov2-embeddings:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F5529912%2F9154108%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240812%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240812T025217Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D79dee6dce645e5cc4de5577e79f961491c9b04d5c8328573ef71c2b55d19b71cce2da8db6cb118859c20003bb5bbeac8ac9ea943e7fac37806e4b8beb9d196c35a0ded5e44946f8c91eba20eb4366001259343762a6415c3a99924add23d501611d2b5ff09923ecc33cd55d9b9cfe94fbb617124bc1bf62a44e363bde59044271786e5e33ec8575a8cc9747d093e195bd0be71e194cf0543b200089051e773535d2aba9e2f77991e3a0e3103f7eff81d97e59ac03732c4bcf1835d35d3c5ab9de14c89196b12f41abbfc364b737d01d64d706c3566245e23ebe981b6f555490b17665ea3975122e20bf610321c483b82025159c27626144e2a040afda9d51ef4'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

"""# Import necessary libraries"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install catboost

from typing import Callable

import numpy as np
import pandas as pd
import torch
import torch_xla.core.xla_model as xm
from catboost import Pool, CatBoostRegressor
from PIL import Image
from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from torchvision import models, transforms
from tqdm import tqdm

"""# Set data paths and seed"""

BASE_DATA_PATH = '/kaggle/input/cs-480-2024-spring/data'
SEED = 42

"""# Load data"""

train_df = pd.read_csv('/kaggle/input/cs-480-2024-spring/data/train.csv')
train_df['image_path'] = train_df['id'].apply(lambda id_: f"{BASE_DATA_PATH}/train_images/{id_}.jpeg")

test_df = pd.read_csv('/kaggle/input/cs-480-2024-spring/data/test.csv')
test_df['image_path'] = test_df['id'].apply(lambda id_: f"{BASE_DATA_PATH}/test_images/{id_}.jpeg")

"""# Split data"""

train_data, val_data = train_test_split(train_df, test_size=0.1, shuffle=True, random_state=SEED)

# After splitting, the original indices from train_df are retained, which can be non-sequential and confusing.
train_data.reset_index(drop=True, inplace=True)
val_data.reset_index(drop=True, inplace=True)

NUM_FEATURES = 164
target_column_names = ['X4_mean', 'X11_mean', 'X18_mean', 'X26_mean', 'X50_mean', 'X3112_mean']
feature_column_names = train_data.columns[:NUM_FEATURES]

"""# Filter outliers"""

def filter_outliers(df: pd.DataFrame, column_names: list[str], lower: float = 0.01, upper: float = 0.99) -> pd.DataFrame:
    lower_quantile = df[column_names].quantile(0.01)
    upper_quantile = df[column_names].quantile(0.99)

    mask = (df[column_names] > lower_quantile) & (df[column_names] < upper_quantile)
    filtered_df = df[mask.all(axis=1)]

    return filtered_df

filtered_train_data = filter_outliers(train_data, target_column_names)
filtered_val_data = filter_outliers(val_data, target_column_names)

"""# Standardize features"""

scaler = StandardScaler()

X_train = scaler.fit_transform(filtered_train_data[feature_column_names].values.astype(np.float32))
X_val = scaler.fit_transform(filtered_val_data[feature_column_names].values.astype(np.float32))
X_test = scaler.transform(test_df[feature_column_names].values.astype(np.float32))

y_train = filtered_train_data[target_column_names].values
y_val = filtered_val_data[target_column_names].values

"""# Device setup"""

# Check for TPU
try:
    device = xm.xla_device()
    print("Using TPU")
except RuntimeError:
    # Fallback to GPU or CPU
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using {device}")

"""# Define transformations and embedding function"""

transform = transforms.Compose([
    transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC),
    transforms.ToTensor(),
    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
])

def get_embeddings(df: pd.DataFrame, transform: Callable, model: Callable, batch_size: int = 64) -> list[np.ndarray]:
    embeddings = []
    image_tensors = []

    for _, row in tqdm(df.iterrows(), total=df.shape[0]):
        image_path = row['image_path']
        image = Image.open(image_path).convert('RGB')
        image_tensor = transform(image).unsqueeze(0).to(device)
        image_tensors.append(image_tensor)

        if len(image_tensors) == batch_size:
            batch_tensor = torch.cat(image_tensors, dim=0)
            with torch.no_grad():
                batch_embeddings = model(batch_tensor).cpu().numpy()
            embeddings.extend(batch_embeddings)
            image_tensors = []

    if image_tensors:
        batch_tensor = torch.cat(image_tensors, dim=0)
        with torch.no_grad():
            batch_embeddings = model(batch_tensor).cpu().numpy()
        embeddings.extend(batch_embeddings)

    return embeddings

"""# Run ablation studies

## ResNet
"""

resnet = models.resnet50(pretrained=True).to(device).eval()

train_embeddings = get_embeddings(df=filtered_train_data, transform=transform, model=resnet)
val_embeddings = get_embeddings(df=filtered_val_data, transform=transform, model=resnet)

poly = PolynomialFeatures(2)
X_train_poly = poly.fit_transform(X_train)
X_val_poly = poly.transform(X_val)
X_test_poly = poly.transform(X_test)

X_train_fused = pd.DataFrame(np.concatenate((X_train_poly, train_embeddings), axis=1))
X_train_fused["embeddings"] = list(train_embeddings)

X_val_fused = pd.DataFrame(np.concatenate((X_val_poly, val_embeddings), axis=1))
X_val_fused["embeddings"] = list(val_embeddings)

column_name_to_r2_score = {}
for idx, column in tqdm(enumerate(target_column_names), total=len(target_column_names)):
    col_y_train = y_train[:, idx]
    col_y_val = y_val[:, idx]

    train_pool = Pool(X_train_fused, col_y_train, embedding_features=["embeddings"])
    val_pool = Pool(X_val_fused, col_y_val, embedding_features=["embeddings"])

    # Train model
    model = CatBoostRegressor(iterations=1500, learning_rate=0.06, loss_function="RMSE", verbose=100, random_state=SEED)
    model.fit(train_pool, plot_file=f"resnet_plot.png")

    # Predict and evaluate
    col_y_val_pred = model.predict(val_pool)
    col_r2_score = r2_score(col_y_val, col_y_val_pred)
    column_name_to_r2_score[column] = col_r2_score

column_name_to_r2_score

np.mean(list(column_name_to_r2_score.values()))

"""Delete ResNet model to free up RAM"""

del resnet

"""## DINOv2"""

dinov2 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14_reg').to(device).eval()

train_embeddings = get_embeddings(df=filtered_train_data, transform=transform, model=dinov2)
val_embeddings = get_embeddings(df=filtered_val_data, transform=transform, model=dinov2)

poly = PolynomialFeatures(2)
X_train_poly = poly.fit_transform(X_train)
X_val_poly = poly.transform(X_val)
X_test_poly = poly.transform(X_test)

X_train_fused = pd.DataFrame(np.concatenate((X_train_poly, train_embeddings), axis=1))
X_train_fused["embeddings"] = list(train_embeddings)

X_val_fused = pd.DataFrame(np.concatenate((X_val_poly, val_embeddings), axis=1))
X_val_fused["embeddings"] = list(val_embeddings)

column_name_to_r2_score = {}
column_name_to_model = {}
for idx, column in tqdm(enumerate(target_column_names), total=len(target_column_names)):
    col_y_train = y_train[:, idx]
    col_y_val = y_val[:, idx]

    train_pool = Pool(X_train_fused, col_y_train, embedding_features=["embeddings"])
    val_pool = Pool(X_val_fused, col_y_val, embedding_features=["embeddings"])

    # Train model
    model = CatBoostRegressor(iterations=1500, learning_rate=0.06, loss_function="RMSE", verbose=100, random_state=SEED)
    model.fit(train_pool, plot=True, plot_file=f"dinov2_plot_{column}")
    column_name_to_model[column] = model

    # Predict and evaluate
    col_y_val_pred = model.predict(val_pool)
    col_r2_score = r2_score(col_y_val, col_y_val_pred)
    column_name_to_r2_score[column] = col_r2_score
    print(f'Column name: {column}, R2 score: {col_r2_score}')

column_name_to_r2_score

np.mean(list(column_name_to_r2_score.values()))

"""# Generate submission CSV"""

submission_col_names = [col_name.replace('_mean', '') for col_name in target_column_names]
submission = pd.DataFrame(
    {
        'id': test_df['id'],
        **{col: 0 for col in submission_col_names}
    }
)

test_embeddings = get_embeddings(df=test_df, transform=transform, model=dinov2)

X_test_poly = poly.fit_transform(X_test)
X_test_fused = pd.DataFrame(np.concatenate((X_test_poly, test_embeddings), axis=1))
X_test_fused["embeddings"] = list(test_embeddings)

for idx, column in enumerate(target_column_names):
    test_pool = Pool(X_test_fused, embedding_features=["embeddings"])
    column_pred = column_name_to_model[column].predict(test_pool)
    submission[column.replace('_mean', '')] = column_pred

submission.to_csv('/kaggle/working/submission.csv', index=False)
submission.head()